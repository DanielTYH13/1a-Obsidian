---
tags:
---
*Key Concepts:*
___
- Linear combinations - linear combinations are a single vector expressed as the scalar multiple of one or more other vectors. 
	- Any vector in R^n can be expressed as a linear combination of another vector given that
		- There a n vectors
		- All vectors are non zero
		- All vectors are not scalar multiples of any other.
	- This is true due to the proof that Any $n$ linearly independent vectors span $\mathbb{R}^n$
- The Norm of a Vector
	- Definiton: 
		- $\vec{x} \in \mathbb{R}^n$
		- $\|\vec{x}\| = sqrt(x_{1}^{2}+\dots+x_{n}^2)$     
	- A vector in $\mathbb{R}^2$ can be divided into its orthogonal components. The hypotenuse that they define is precisely the vector. By taking the root of the sum of the squares of the components, therefore, you are left with the magnitude of the vector without its direction. 
		- Since the pythagorean theorem extends to $\mathbb{R}^n$, this form of the norm can be used on any vector. 
	- Properties of the Norm:
		- The norm of the vector, given that it is non-zero, must be greater than 0. If it is a zero vector, the norm of the vector is 0. The proof of thise is trivial.
			- $\| \vec{x} \| \ge 0$ 
			- $\| \vec{x} \| = 0 \iff \vec{x} = \vec{0}$ 
		- The norm of a scalar multiple of a vector is the absolute of the scalar multiple multiplied with the norm of the vector:
			- $$
\begin{split} 
\| c\vec{x} \| & = \sqrt{(cx_{1})^{2} + \dots + (cx_{n})^{2}} \\ 
& =  \sqrt{c^{2}x_{1}^{2} + \dots + c^{2}x_{n}^{2}}  \\
& = \sqrt{c^{2}(x_{1}^{2}+\dots+x_{n}^{2})} \\
& = |c|\sqrt{x_{1}^{2}+\dots+x_{n}^{2}} \quad \text{C is now absoluted, as the root of a square is always positive.}
\end{split}
$$

*Significant Theorems:*
___
- Fundamental Properties of Linear Algebra:  Let $\vec{w}, \vec{x}, \vec{y} \in \mathbb{R}^{n}$, and $c, d \in \mathbb{R}$
	- V1: $\vec{x} + \vec{y}  \in \mathbb{R}^n$: Vector addition is restricted to the Reals. Meaning, no matter what two real vectors you add to each other, it will never leave the set of R^n  
	- V2: $\vec{x} + \vec{y} = \vec{y} + \vec{y}$: Vector addition is commutative. Geometrically, this is proven by showing that a parallelogram, regardless of which leg you traverse, will lead you to the same point.  ![[Pasted image 20250916183048.png]]
		- Algebraically, this is proven by the following $$\begin{split}\vec{x} + \vec{y} & = \begin{bmatrix}x_{1} + y_1 \\ \vdots\\ x_{n} + y_{n} \end{bmatrix} \\ \vec{x} + \vec{y} & = \begin{bmatrix} y_{1} + x_{1} \\ \vdots \\ y_{n}+ x_{n}\end{bmatrix} \\ & = \vec{y} + \vec{x} \end{split}$$
		- This is derived through scalar commutivity for each individual component.
	- V3: Vector addition is associative: $\vec{x} + (\vec{y} + \vec{w}) = (\vec{x} + \vec{y}) + \vec{w}$. Regardless of the order you put any given n vectors, they will all point to the same place. 
		- This follows a similar proof to vector commutivity, but instead employs the associative property of vectors:  $$
\begin{split} 
\vec{x} + (\vec{y} + \vec{w}) & = 
\begin{bmatrix} x_{1} + (y_{1}+w_{1}) \\ \vdots \\ x_{n}+(y_{n}+w_{n})) \end{bmatrix} \\ 
& = 
\begin{bmatrix} (x_{1} + y_{1})+w_{1} \\ \vdots \\ (x_{n}+y_{n})+w_{n}) \end{bmatrix} \\ 
& = (\vec{x} + \vec{y}) + \vec{w})
\end{split} \\ 
$$
	- V4: Scalar Multiplication in $\mathbb{R}^n$ is closed. Meaning that if you multiply two vectors in $\mathbb{R}^n$, it will never leave this set.
	- V5: $c(d\vec{x}) = (cd)\vec{x}$. This can be proven by expanding into components, and then using the associative property on the scalar multiples, and the component. 
	- V6: $(c+d)\vec{x} = c\vec{x} + d\vec{x}$. This can be proved, again, by expanding into components, and then using the scalar distributive property to achieve the RHS.
	- V7: $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$. This can be proved, again, by expanding into components, where $c(x_{n}+y_{n)}=cx_{n}+cy_{n}$.


*Respective Proofs*
___

*Common Mistakes:*
___

*Terms and Definitions:*
___

